---
title: "Local dev responds with test message in `UI` (#10)"
categories: [agents, 25.05, release]
sidebar: release-notes
toc-expand: true
date: "April 28, 2025"
# CHECK: Title validation failed - FAIL: Is not properly capitalized and punctuated.
# Content edited by AI - 2025-06-05 17:37
# Content validated by AI - 2025-06-05 17:37
# Content overwritten from an earlier version - 2025-06-05 17:37
# PR URL: https://github.com/validmind/agents/pull/10
---

![image](https://github.com/user-attachments/assets/433e6e73-dd0e-4714-a514-e226375669ad)

When running locally, we have logic that checks if litellm is running by sending a single "hello" to the llm. This is a great way to be able to fall back to the bare openai api if the developer doesn't want to run litellm locally. However the problem is that this invoke to the langchain llm client happens within the task node of the langgraph workflow. Meaning that the "Hello" back from the LLM is streamed to the UI. The solution is simply to remove the callbacks array when running `client.invoke()`.