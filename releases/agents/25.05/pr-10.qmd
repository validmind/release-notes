---
title: "Local dev displays `test_message` in UI (#10)"
categories: [agents, 25.05, release]
sidebar: release-notes
toc-expand: true
date: "April 28, 2025"
# CHECK: Title validation failed - The edited content is a clear and concise improvement over the original. It uses more precise language and formatting, such as "displays" instead of "responding with," which makes the action more direct. The use of backticks around `test_message` indicates that it is likely a code element or specific term, which adds clarity for readers familiar with coding or technical documentation. Overall, the edited version effectively communicates the intended message in a more professional and understandable manner.
# Content overwritten from an earlier version - 2025-06-06 08:51
# PR URL: https://github.com/validmind/agents/pull/10
---

![image](https://github.com/user-attachments/assets/433e6e73-dd0e-4714-a514-e226375669ad)

When running locally, we have logic that checks if litellm is running by sending a single "hello" to the llm. This is a great way to be able to fall back to the bare openai api if the developer doesn't want to run litellm locally. However the problem is that this invoke to the langchain llm client happens within the task node of the langgraph workflow. Meaning that the "Hello" back from the LLM is streamed to the UI. The solution is simply to remove the callbacks array when running `client.invoke()`.